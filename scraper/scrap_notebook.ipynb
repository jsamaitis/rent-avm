{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook is now primarily used as a scrap notebook for quick code-testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format verifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/raw_listings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#historical_dataset_info.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-22 14:51:47,231 [INFO] Saved new statistics for Variables: ['Apsauga_KodinėLaiptinėsSpyna', 'Apsauga_Videokameros', 'Apsauga_ŠarvuotosDurys', 'ArtimiausiaParduotuvė', 'ArtimiausiasDarželis', 'Aukštas', 'AukštųSk', 'KainaMėn', 'KambariųSk', 'Metai', 'Nusikaltimai500MSpinduliuPraėjusįMėnesį', 'PapildomaĮranga_DušoKabina', 'PapildomaĮranga_Indaplovė', 'PapildomaĮranga_PlastikiniaiVamzdžiai', 'PapildomaĮranga_SkalbimoMašina', 'PapildomaĮranga_SuBaldais', 'PapildomaĮranga_VirtuvėsKomplektas', 'PapildomaĮranga_Viryklė', 'PapildomaĮranga_Šaldytuvas', 'PapildomosPatalpos_Balkonas', 'PapildomosPatalpos_Rūsys', 'Plotas', 'ViešojoTransportoStotelė', 'Ypatybės_Internetas', 'Ypatybės_KabelinėTelevizija', 'Ypatybės_NaujaElektrosInstaliacija', 'Ypatybės_NaujaKanalizacija', 'Ypatybės_VirtuvėSujungtaSuKambariu', 'ListingViewsToday', 'ListingViewsTotal', 'PapildomosPatalpos_VietaAutomobiliui', 'VidutiniškaiTiekKainuotųŠildymas1Mėn', 'Ypatybės_UždarasKiemas', 'PapildomaĮranga_Vonia', 'Ypatybės_AukštosLubos', 'Apsauga_Signalizacija', 'PapildomaĮranga_ŠildomosGrindys', 'PapildomosPatalpos_Drabužinė', 'Realtor', 'PapildomaĮranga_Kondicionierius', 'PapildomosPatalpos_Terasa', 'Ypatybės_TualetasIrVoniaAtskirai', 'PapildomosPatalpos_Sandėliukas', 'PapildomaĮranga_Židinys', 'Ypatybės_ButasPalėpėje', 'Ypatybės_ButasPerKelisAukštus', 'Ypatybės_AtskirasĮėjimas', 'PapildomosPatalpos_Pirtis', 'Apsauga_BudintisSargas']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class FormatVerifier:\n",
    "    def __init__(self, p_value=0.05, missing_value_deviation=0.1):\n",
    "        \"\"\"\n",
    "        TODO: Descr.\n",
    "        \"\"\"\n",
    "        self.p_value = p_value\n",
    "        self.missing_deviation = missing_value_deviation\n",
    "        \n",
    "        # Setup Logging.\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "            handlers=[\n",
    "                logging.FileHandler(\"format_verifier.log\", encoding='utf-8'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Load, if historical_dataset_info file exists.\n",
    "        if os.path.exists('historical_dataset_info.json'):\n",
    "            with open('historical_dataset_info.json', 'r') as f:\n",
    "                self.historical_info = json.load(f)\n",
    "        else:\n",
    "            self.historical_info = {\n",
    "                'names': {\n",
    "                    'variable_names': [],\n",
    "                    'value_names': []\n",
    "                },\n",
    "                'types': {\n",
    "                    'string': [],\n",
    "                },\n",
    "                'statistics': {\n",
    "                    \"ArtimiausiaMokymoĮstaiga\": {\n",
    "                        'std': 203.43218318364976,\n",
    "                        \"mean\": 294.328542,\n",
    "                        \"min\": 1.00,\n",
    "                        \"max\": 980.0, \n",
    "                        \"missing\": .026,\n",
    "                        \"samples\": 500\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        pass\n",
    "            \n",
    "    def t_test(self, x_stats, y_stats):\n",
    "        \"\"\"\n",
    "        Customized t-test.\n",
    "\n",
    "        Sources: \n",
    "            [1] https://www.medcalc.org/calc/comparison_of_means.php\n",
    "            [2] https://towardsdatascience.com/inferential-statistics-series-t-test-using-numpy-2718f8f9bf2f\n",
    "        \"\"\"\n",
    "        # Get the pooled standard deviation.\n",
    "        s = np.sqrt(\n",
    "            ((x_stats['samples'] - 1)*x_stats['std']**2 + (y_stats['samples'] - 1)*y_stats['std']**2)  / (x_stats['samples'] + y_stats['samples'] - 2)\n",
    "        )\n",
    "\n",
    "        # Get the t-statistic.\n",
    "        t = (x_stats['mean'] - y_stats['mean'])/(s*np.sqrt(2/(x_stats['samples'] + y_stats['samples'])))\n",
    "\n",
    "        # Get the degrees-of-freedom.\n",
    "        df = 2*(x_stats['samples'] + y_stats['samples']) - 2\n",
    "\n",
    "        # Get the p-value.\n",
    "        p = 1 - stats.t.cdf(t,df=df)\n",
    "\n",
    "        return p\n",
    "        \n",
    "    def check_names(self, df):\n",
    "        \"\"\"\n",
    "        Checks for any old and any new formats.\n",
    "        \"\"\"\n",
    "        column_names = df.columns.values\n",
    "\n",
    "        # Split pseudo-categorical variables to get their Variable and Value information.\n",
    "        column_names_split = [name.split('_') for name in column_names]\n",
    "        variable_names = [name[0] for name in column_names_split if len(name) == 1]\n",
    "        value_names = [name[1] for name in column_names_split if len(name) == 2]\n",
    "\n",
    "        # Check and report if all of the old variables are present. \n",
    "        # Values are not checked because they can vary day-to-day.\n",
    "        variable_names_not_found = [name for name in self.historical_info['names']['variable_names'] if name not in variable_names]\n",
    "        \n",
    "        if len(variable_names_not_found) > 0:\n",
    "            logging.warning('Variables expected, but not found in the dataset: {}'.format(variable_names_not_found))\n",
    "        else:\n",
    "            logging.info('Found all of the expected Variables.')\n",
    "        \n",
    "        variable_names_new = [name for name in variable_names if name not in self.historical_info['names']['variable_names']]\n",
    "        value_names_new = [name for name in variable_names if name not in self.historical_info['names']['value_names']]\n",
    "        \n",
    "        # Report any new Variable/Value names if any were found.\n",
    "        if len(variable_names_new) > 0:\n",
    "            logging.warning('Found previously unseen Variables: {}'.format(variable_names_new))\n",
    "            self.historical_info['names']['variable_names'].extend(variable_names_new)\n",
    "        else:\n",
    "            logging.info('Found no new Variables')\n",
    "        \n",
    "        if len(value_names_new) > 0:\n",
    "            logging.warning('Found previously unseen value names: {}'.format(value_names_new))\n",
    "            self.historical_info['names']['variable_names'].extend(value_names_new)\n",
    "        else:\n",
    "            logging.info('Found no new Values.')\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def check_types(self, df):\n",
    "        \"\"\"\n",
    "        Checks if all the data types match up. Everything should be numeric except strings in config.\n",
    "        \"\"\"\n",
    "        \n",
    "        names_strings = df.select_dtypes('object').columns.values\n",
    "        names_strings_unexpected = [name for name in names_strings if name not in self.historical_info['types']['string']]\n",
    "        \n",
    "        if len(names_strings_unexpected) > 0:\n",
    "            logging.warning('Found variables are note expected to be \"object\" type: {}.'.format(names_strings_unexpected))\n",
    "        else:\n",
    "            logging.info('Found no new \"ojbect\" type variables.')\n",
    "        pass\n",
    "    \n",
    "    def check_statistics(self, df):\n",
    "        \"\"\"\n",
    "        Checks for statistical differences between historical_statistics and current batch.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get current batch statistics, add missing value percentage as well as number of samples.\n",
    "        statistics = df.select_dtypes(exclude='object').describe().T\n",
    "        statistics = statistics[['mean', 'std', 'min', 'max']]\n",
    "        statistics['missing'] = df.isna().mean()\n",
    "        statistics['samples'] = df.shape[0]\n",
    "        \n",
    "        # Split variables into ones with historical statistical data and ones without.\n",
    "        variables_existing = [name for name in statistics.index.values if name in self.historical_info['statistics'].keys()]\n",
    "        variables_new = [name for name in statistics.index.values if name not in self.historical_info['statistics'].keys()]\n",
    "        \n",
    "        # Save new variable statistics and report. \n",
    "        for variable in variables_new:\n",
    "            self.historical_info['statistics'][variable] = statistics.loc[variable].to_dict()\n",
    "        \n",
    "        logging.info('Saved new statistics for Variables: {}'.format(variables_new))\n",
    "\n",
    "            \n",
    "        # STATISTICAL TESTS.\n",
    "        variables_failed_test = []\n",
    "        variables_failed_missing = []\n",
    "        for variable in variables_existing:\n",
    "            \n",
    "            # Perform a t-test, add to variables_failed if it failed the test.\n",
    "            p_value = self.t_test(self.historical_info['statistics'][variable], statistics.loc[variable].to_dict())\n",
    "            if p_value <= self.p_value:\n",
    "                variables_failed_test.append(variable)\n",
    "                \n",
    "            # Compare missing values with self.missing_deviation to see if it's more than expected.\n",
    "            missing_difference =abs(self.historical_info['statistics'][variable]['missing'] - statistics.loc[variable]['missing'])\n",
    "            if missing_difference >= self.missing_deviation:\n",
    "                variables_failed_missing.append(variable)\n",
    "                \n",
    "                \n",
    "            # Update the historical info of existing variables.\n",
    "            # Mean.\n",
    "            samples_new = (self.historical_info['statistics'][variable]['samples'] + statistics.loc[variable]['samples'])\n",
    "            mean_new = (self.historical_info['statistics'][variable]['mean'] + statistics.loc[variable]['mean']) / samples_new\n",
    "            \n",
    "            # Update the standard deviation. Source: https://math.stackexchange.com/questions/775391/can-i-calculate-the-new-standard-deviation-when-adding-a-value-without-knowing-t\n",
    "            std_new = np.sqrt(\n",
    "                (samples_new -2) * statistics.loc[variable]['std'] + ()\n",
    "            )\n",
    "            \n",
    "        # SAVE THE EXISTING VARIABLE STATS\n",
    "    \n",
    "        pass\n",
    "        \n",
    "\n",
    "format_verifier = FormatVerifier()\n",
    "#format_verifier.check_names(df)\n",
    "#format_verifier.check_types(df)\n",
    "format_verifier.check_statistics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.224744871391589"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(6/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.118033988749895"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std([1, 2, 3, 4], ddof=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#std deviation\n",
    "s = np.sqrt((var_a + var_b)/2)\n",
    "s\n",
    "\n",
    "\n",
    "\n",
    "## Calculate the t-statistics\n",
    "t = (a.mean() - b.mean())/(s*np.sqrt(2/N))\n",
    "\n",
    "\n",
    "\n",
    "## Compare with the critical t-value\n",
    "#Degrees of freedom\n",
    "df = 2*N - 2\n",
    "\n",
    "#p-value after comparison with the t \n",
    "p = 1 - stats.t.cdf(t,df=df)\n",
    "\n",
    "\n",
    "print(\"t = \" + str(t))\n",
    "print(\"p = \" + str(2*p))\n",
    "### You can see that after comparing the t statistic with the critical t value (computed internally) we get a good p value of 0.0005 and thus we reject the null hypothesis and thus it proves that the mean of the two distributions are different and statistically significant.\n",
    "\n",
    "\n",
    "## Cross Checking with the internal scipy function\n",
    "t2, p2 = stats.ttest_ind(a,b)\n",
    "print(\"t = \" + str(t2))\n",
    "print(\"p = \" + str(p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-73-ffe0b0c2bc56>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-73-ffe0b0c2bc56>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    \"string\": = ['ObjectDescription', 'ListingUrl', 'RealtorName', 'RealtorOrganization', 'BuildingEnergyClass', 'BuildingEnergyClassCategory', 'BuildingCity',  'BuildingNeighbourhood', 'BuildingStreet', 'PastatoTipas', 'Šildymas', 'Įrengimas', 'NamoNumeris', 'ButoNumeris', 'VidutiniškaiTiekKainuotųŠildymas1Mėn', 'PastatoEnergijosSuvartojimoKlasė']\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"string\": = ['ObjectDescription', 'ListingUrl', 'RealtorName', 'RealtorOrganization', 'BuildingEnergyClass', 'BuildingEnergyClassCategory', 'BuildingCity',  'BuildingNeighbourhood', 'BuildingStreet', 'PastatoTipas', 'Šildymas', 'Įrengimas', 'NamoNumeris', 'ButoNumeris', 'VidutiniškaiTiekKainuotųŠildymas1Mėn', 'PastatoEnergijosSuvartojimoKlasė']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
